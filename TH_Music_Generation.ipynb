{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purekikan/Thai-Traditional-Music-Generation-with-LSTM/blob/main/TH_Music_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIiktP8TjUfC"
      },
      "source": [
        "#© MIT 6.S191: Introduction to Deep Learning\n",
        "http://introtodeeplearning.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT-AupDOSBbK"
      },
      "source": [
        "# Dependency & Defining Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajab7GlDSWi8",
        "outputId": "eb8cdf84-097a-4187-8203-eda0c1fd4b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mitdeeplearning\n",
            "  Downloading mitdeeplearning-0.2.0.tar.gz (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 34.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning) (4.64.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from mitdeeplearning) (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning) (5.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->mitdeeplearning) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->mitdeeplearning) (3.11.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.2.0-py3-none-any.whl size=2115441 sha256=012b4fbed67bd9d3f37cc905bb5375e2af582fb3a1e01d223243d92af1c9ed05\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/45/44/c5b304f31f37e8d2315f9e969fd8cdb0014a5c28608d0bf410\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.20.9\n",
            "  Downloading Levenshtein-0.20.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 45.5 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 53.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.20.9 python-Levenshtein-0.20.9 rapidfuzz-2.13.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mutagen\n",
            "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 24.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: mutagen\n",
            "Successfully installed mutagen-1.46.0\n"
          ]
        }
      ],
      "source": [
        "#for training and data preprocessing\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf \n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for evaluating\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "import fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "!pip install mutagen\n",
        "import mutagen\n",
        "from mutagen.wave import WAVE\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from datetime import date\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I9LfcB1TztF"
      },
      "outputs": [],
      "source": [
        "def LSTM(rnn_units): \n",
        "  return tf.keras.layers.LSTM(\n",
        "    rnn_units, \n",
        "    return_sequences=True, \n",
        "    recurrent_initializer='glorot_uniform',\n",
        "    recurrent_activation='sigmoid',\n",
        "    stateful=True,\n",
        "  )\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    LSTM(rnn_units), \n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  return loss\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_hat = model(x)\n",
        "    loss = compute_loss(y, y_hat)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optmz[param_set].apply_gradients(zip(grads, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def generate_text(model, start_string, generation_length=1000):\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "  tqdm._instances.clear()\n",
        "\n",
        "  for i in tqdm(range(generation_length)):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(idx2char[predicted_id])    \n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "def reset_weights(model):\n",
        "  for layer in model.layers: \n",
        "    if isinstance(layer, tf.keras.Model):\n",
        "      reset_weights(layer)\n",
        "      continue\n",
        "    for k, initializer in layer.__dict__.items():\n",
        "      if \"initializer\" not in k:\n",
        "        continue\n",
        "      # find the corresponding variable\n",
        "      var = getattr(layer, k.replace(\"_initializer\", \"\"))\n",
        "      var.assign(initializer(var.shape, var.dtype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZlLysx-TWno"
      },
      "outputs": [],
      "source": [
        "def get_fzwz(generated_text, songs):\n",
        "  fzwzscore_f = []\n",
        "  fzwzscore_p = []\n",
        "  for generated_song in mdl.lab1.extract_song_snippet(generated_text):\n",
        "    \n",
        "    full = [s[-1] for s in process.extract(generated_song, songs, scorer=fuzz.ratio, limit = len(songs))]\n",
        "    partial = [s[-1] for s in process.extract(generated_song, songs, scorer=fuzz.partial_ratio, limit = len(songs))]\n",
        "\n",
        "    fzwzscore_f.append(sum(full)/len(full))\n",
        "    fzwzscore_p.append(sum(partial)/len(partial))\n",
        "  return fzwzscore_f, fzwzscore_p\n",
        "  \n",
        "class mp3_converter():\n",
        "    def __init__(self, path, ext, dirName):\n",
        "        \"\"\"Class that takes folder of music files of one file type, \n",
        "        converts them to mp3 and creates a new directory and moves them into it\n",
        "        Input path of files that you would like to convert\n",
        "        Extension of files you would like to convert i.e. WAV\n",
        "        Folder name of the new directory you would like to create\"\"\"\n",
        "        self.path = path\n",
        "        self.ext = ext\n",
        "        self.dirName = dirName\n",
        "\n",
        "    def lower_underscore(self):\n",
        "        \"\"\"\n",
        "        Converts all files in path to lowercase\n",
        "        Replaces all spaces in filename with _\n",
        "        \"\"\"\n",
        "        directory = self.path\n",
        "        [os.rename(os.path.join(directory, f), os.path.join(directory, f).replace(' ', '_').lower()) for f in os.listdir(directory)]\n",
        "\n",
        "    def mp3(self):\n",
        "        \"\"\"\n",
        "        Converts all files in path with entered extension to mp3\n",
        "        \"\"\"\n",
        "        directory = self.path\n",
        "\n",
        "        for f in os.listdir(directory):\n",
        "            if (f.endswith(self.ext)):\n",
        "                os.system(\"ffmpeg -i {} -ar 44100 -ac 2 -b:a 192k {}/{}.mp3\".format(\n",
        "                    os.path.join(directory, f), directory, os.path.splitext(f)[0]))\n",
        "\n",
        "    def make_dir(self):\n",
        "        \"\"\"\n",
        "        Creates a directory for mp3's and moves all \n",
        "        previously created mp3's into it and moves the directory up one\n",
        "        \"\"\"\n",
        "        mp3_directory = self.path + \"/\" + self.dirName\n",
        "        if not os.path.exists(mp3_directory):\n",
        "            os.makedirs(mp3_directory)\n",
        "        for filename in os.listdir(self.path):\n",
        "            if (filename.endswith(\".mp3\")):\n",
        "                source = os.path.join(self.path, filename)\n",
        "                dest = shutil.copy(source, mp3_directory)\n",
        "\n",
        "def evaluate_song(generated_text, param_set, train_set, date = date.today()):\n",
        "  n = 0\n",
        "  l = []\n",
        "  generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "  for i, song in enumerate(generated_songs):\n",
        "    waveform = mdl.lab1.play_song(song)\n",
        "    if waveform:\n",
        "      print(\"Generated song\", i+1)\n",
        "      \n",
        "      song_name = f'{date}--{param_set}.{train_set}.{n+1}.wav'\n",
        "      \n",
        "      with open(song_name, 'wb') as f:\n",
        "        f.write(waveform.data)\n",
        "      audio = WAVE(song_name)\n",
        "      length = int(audio.info.length)\n",
        "      l.append(length)\n",
        "      n+=1\n",
        "\n",
        "  wav_play_rate = n/len(generated_songs)*100\n",
        "  gensonglen_avg = sum(l)/len(l)\n",
        "  gensonglen_min = min(l)\n",
        "  \n",
        "  return wav_play_rate, gensonglen_avg, gensonglen_min\n",
        "\n",
        "def save_song(dir = '/content/mp3'):\n",
        "  conv = mp3_converter('/content', \".wav\", dir)\n",
        "  conv.lower_underscore()\n",
        "  conv.mp3()\n",
        "  conv.make_dir()\n",
        "\n",
        "def save_config(param_set, train_set, date=date.today()):\n",
        "  ckpt_dir = f'drive/MyDrive/Project_M.5/Project-M.5/LSTM/ckpt/{date}--{param_set}.{train_set}/'\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "  for i in os.listdir(\"/content/training_checkpoints\"):\n",
        "    shutil.copy(\"/content/training_checkpoints/\" + i, '/content/' + ckpt_dir + i)\n",
        "  save_song(ckpt_dir)\n",
        "  mp3_wavs = glob.glob(\"./*.wav\")\n",
        "  mp3_wavs.extend(glob.glob(\"./*.mp3\"))\n",
        "  for x in mp3_wavs:\n",
        "    os.remove(x)\n",
        "\n",
        "def train(num_training_iterations):\n",
        "  for iter in tqdm(range(num_training_iterations)):\n",
        "    x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    if iter % 100 == 0:     \n",
        "      model.save_weights(checkpoint_prefix)\n",
        "  model.save_weights(checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IroKcopSIlB"
      },
      "source": [
        "# Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgLIMzTlN5hz",
        "outputId": "a8518784-fc37-481e-ac38-48695b5ca70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Thai-Traditional-Music-Generation-with-LSTM'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 74\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/purekikan/Thai-Traditional-Music-Generation-with-LSTM.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmXU2MAVOwrh"
      },
      "outputs": [],
      "source": [
        "songs = []\n",
        "n=0\n",
        "\n",
        "for x in glob.glob(\"/content/Thai-Traditional-Music-Generation-with-LSTM/Dataset/*.mid\"):\n",
        "  abc = !midi2abc {x} -k -2 -m 2/2 -title {n}\n",
        "  abc_list = []\n",
        "  for x in abc:\n",
        "    if not x.startswith((\"V:\", \"%\")):\n",
        "      abc_list.append(x.split(' %')[0])\n",
        "  \n",
        "  abc_prcesd = f'X: {n}'+'\\n'.join(abc_list)[4:]+'!'\n",
        "  songs.append(abc_prcesd)\n",
        "  n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bimRLtDNV38n",
        "outputId": "aedc6535-1e0f-467a-ae14-b57e1c2dfdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 45 unique characters in the dataset\n"
          ]
        }
      ],
      "source": [
        "songs_joined = '\\n\\n'.join(songs)\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4a4D4OQSfeY"
      },
      "source": [
        "# Process the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf3g0o4gS4OS"
      },
      "outputs": [],
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "def vectorize_string(string):\n",
        "  vectorized_output = np.array([char2idx[char] for char in string])\n",
        "  return vectorized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1VKcQHcymwb"
      },
      "outputs": [],
      "source": [
        "vectorized_songs = vectorize_string(songs_joined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF-N8F7BoDRi"
      },
      "outputs": [],
      "source": [
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "  idx = np.random.choice(n-seq_length, batch_size)\n",
        "  input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
        "  output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
        "  x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "  y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "  return x_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-43F0AdUCYd"
      },
      "source": [
        "# Train the model and generate songs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lTutGO9sVRgT"
      },
      "outputs": [],
      "source": [
        "#@title Setting Parameters\n",
        "num_training_iterations = 2000\n",
        "batch_size = 40\n",
        "seq_length = 115\n",
        "learning_rate = 5.00e-03\n",
        "embedding_dim = 256\n",
        "rnn_units = 600\n",
        "generation_length = 5000\n",
        "param_set = 1\n",
        "vocab_size = len(vocab)\n",
        "optmz = {}\n",
        "optmz[1] = tf.keras.optimizers.Adam(learning_rate)\n",
        "optimizer = optmz[1]\n",
        "start_string = \"X: 0\\nT: 0\\nM: 2/2\\nL: 1/8\\nQ:1/4=100\\nK:Bb\"\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ea-sIDRXXr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "score = 0\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'):\n",
        "  tqdm._instances.clear()\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "print(\"--------Start Training--------\")\n",
        "for iter in tqdm(range(num_training_iterations)):\n",
        "  x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
        "  loss = train_step(x_batch, y_batch)\n",
        "  history.append(loss.numpy().mean())\n",
        "  plotter.plot(history)\n",
        "\n",
        "  if iter % 100 == 0:     \n",
        "    model.save_weights(checkpoint_prefix)\n",
        "model.save_weights(checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ws7-_dek8j_8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate Music\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "generated_text = generate_text(model, start_string=start_string, generation_length= generation_length)\n",
        "fzwz = get_fzwz(generated_text, songs)\n",
        "try:\n",
        "  other_metrics = evaluate_song(generated_text, \"test\", \"test\")\n",
        "except:\n",
        "  print('no song is generated')\n",
        "  other_metrics = [0,0,0]\n",
        "  fzwz = [[0],[0]]\n",
        "save_song(\"mp3\")\n",
        "i = 0\n",
        "for song_name in glob.glob(\"./mp3/*.mp3\"):\n",
        "  i+=1\n",
        "  print(f'----------Playing Song {i}----------')\n",
        "  display(Audio(song_name))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KT-AupDOSBbK",
        "3IroKcopSIlB",
        "q4a4D4OQSfeY"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}